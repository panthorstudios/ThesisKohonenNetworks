# CHAPTER 5 - HISTORY OF NEURAL NETWORKS

##Foundations

###Principles of Psychology

In 1936, the prominent mathematician Alan Turing first used the brain as acomputing paradigm.  Several years later, in 1943, Warren McCulloch and Walter Pitts wrote apaper explaining a possible way that neurons might work.  They made five majorassumptions about neuron function (Eberhard and Dobbins 1990a, p. 15):

1. The activity of the neuron is an "all-or-none" process.
2. A certain fixed number of synapses must be excited within the period oflatent addition in order to excite a neuron at any time, and this number isindependent of previous activity and position on the neuron.
3. The only significant delay within the nervous system is synaptic delay.
4. The activity of any inhibitory synapse absolutely prevents excitation of theneuron at that time.
5. The structure of the net does not change with time.

Of the five assumptions, only the fifth has any relevance to current theory.  Becauseof this, it would seem that the model has no significance.  In fact, the paper would havefallen into obscurity if not for John Von Neumann, the Hungarian-born mathematician that used the paper as a means to teach a course on the theory of computing.  A very important consequence of the McCulloch-Pitts paper is that it formed the basis for subsequent models.

###The Organization of Behavior

1. The information in a neural network is stored inthe synapses (connections).
2. A connection weight learning rate was postulated,proportional to the product of the activation valuesof the neurons.
3. Connection weights are symmetric.
4. As learning occurs, strengths and patterns ofsynapse connections change, and assemblies of cells arecreated by these changes.
The fourth contribution has become known as Hebb's learning rule, which statessimply that a neural pathway is reinforced each time it is used.  All four of thesecontributions have made their way into current networks at some level.

##Early Work

In the 1950s there was a rapid increase in hardware and software technology.This facilitated simulation of neural networks, and thus existing theories were able to be tested. Nathaniel Rochester and other IBM researchers used Hebb's work as a basis for asimulation.  Although the first attempts failed, after some changes the model wassuccessfully tested (Nelson and Illingworth 1991).

##The Computer and the Brain

on a computer, specifically an IBM 704 belonging to Cornell.  Two years later in 1959, Bernard Widrow and Marcian Hoff developed ADALINE (ADAptive LINearElement) and MADALINE (Multiple ADALINE).  MADALINE was the first neural network to be applied to a real-world problem, namely, that of eliminating echoes in telephone lines. Another neural network, the Avalanche network, was developed in 1967 byStephen Grossberg.  The Avalanche network came about as a result of extensivephysiological research on the part of Grossberg.

##Downfall

##Perceptrons

Despite this occurrence, some researchers held their ground.  For example,James Anderson, a neurophysiologist, designed his Brain-State-in-a-Box model, TeavoKohonen developed his associative memory model, and Kunihiko Fukushima of Japandeveloped the Neocognitron model.  These were just a few of the notable developments duringthat period.

##Resurgence

Neural network research was revived somewhat in 1982 with the neural network model proposed by John Hopfield of the California Institute of Technology. Since Hopfield was and still is a well-known and respected physicist, his interest in neural networks gave some credibility to the field.  In the same year, the United States and Japan held the Joint Conference on Cooperative/Competitive Neural Networks. Hopfieldfollowed up his 1982 paper with another paper in 1984.  AT&T Bell Labs produced a hardware neural network chip in 1986 based on Hopfield's work.

##Parallel DistributedProcessing


##Current Status

Since 1987, the range of neural network applications has grown greatly.  In addition, neural networks are being implemented on personal computers to a greater degree than ever before.  Some innovative applications include music composition, stock-market prediction, and monitoring of brain activity.  The dominant network models used are the backpropagation network and self-organizing network, although other models are used as well.
